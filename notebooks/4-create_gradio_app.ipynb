{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from huggingface_hub import from_pretrained_keras\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"mmenendezg/vit_pneumonia_classifier\"\n",
    "IMG_SIZE = [224, 224]\n",
    "IMG_CLASSES = [\"Normal\", \"Pneumonia\"]\n",
    "THRESHOLD = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attention(\n",
    "    attention_vals: tf.Tensor,\n",
    "    n_heads: int,\n",
    "    h_featmap: int,\n",
    "    w_featmap: int,\n",
    "    h_original: int,\n",
    "    w_original: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the attention weights for visualization.\n",
    "\n",
    "    Args:\n",
    "        attention_vals: The attention weights.\n",
    "        n_heads: The number of attention heads.\n",
    "        h_featmap: The height of the feature map.\n",
    "        w_featmap: The width of the feature map.\n",
    "        h_original: The height of the original image.\n",
    "        w_original: The width of the original image.\n",
    "\n",
    "    Returns:\n",
    "        The processed attention weights.\n",
    "    \"\"\"\n",
    "    # We only keep the output patch attention\n",
    "    attention = tf.expand_dims(attention_vals, axis=0)\n",
    "    attention = tf.reshape(attention[0, :, 0, 1:], (n_heads, w_featmap, h_featmap, 1))\n",
    "    # Aggregation of the n heads in the last layer\n",
    "    attention = tf.reduce_mean(attention, axis=0)\n",
    "    attention = tf.image.resize(\n",
    "        attention, size=[h_original, w_original], method=\"lanczos5\"\n",
    "    )\n",
    "    # Normalize the attention values to have values from zero to one\n",
    "    attention -= tf.reduce_min(attention)\n",
    "    attention /= tf.reduce_max(attention)\n",
    "    return attention\n",
    "\n",
    "\n",
    "def get_attention(\n",
    "    attentions: tf.Tensor,\n",
    "    examples: int,\n",
    "    num_attention_heads: int,\n",
    "    h_featmap: int,\n",
    "    w_featmap: int,\n",
    "    h_original: int,\n",
    "    w_original: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the attention weights for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        attentions: The attention weights.\n",
    "        examples: The number of examples.\n",
    "        num_attention_heads: The number of attention heads.\n",
    "        h_featmap: The height of the feature map.\n",
    "        w_featmap: The width of the feature map.\n",
    "        h_original: The height of the original image.\n",
    "        w_original: The width of the original image.\n",
    "\n",
    "    Returns:\n",
    "        The attention weights.\n",
    "    \"\"\"\n",
    "    attentions = tf.reshape(attentions, (examples, num_attention_heads, -1))\n",
    "    last_dimension = int(tf.math.sqrt(float(attentions.shape[-1])).numpy())\n",
    "    attentions = tf.reshape(\n",
    "        attentions, (examples, num_attention_heads, last_dimension, last_dimension)\n",
    "    )\n",
    "\n",
    "    attention_list = []\n",
    "    for attention in attentions:\n",
    "        processed_attenttion = process_attention(\n",
    "            attention, num_attention_heads, h_featmap, w_featmap, h_original, w_original\n",
    "        )\n",
    "        attention_list.append(processed_attenttion)\n",
    "    return attention_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_preprocessor():\n",
    "    image_preprocessor = Sequential(\n",
    "        [\n",
    "            layers.Resizing(\n",
    "                height=IMG_SIZE[0],\n",
    "                width=IMG_SIZE[1],\n",
    "                interpolation=\"nearest\",\n",
    "            ),\n",
    "            layers.Rescaling(scale=1.0 / 255.0),\n",
    "        ]\n",
    "    )\n",
    "    return image_preprocessor\n",
    "\n",
    "\n",
    "def get_attention_image(attentions: [tf.Tensor], image: PIL.Image) -> PIL.Image:\n",
    "    rescaled_image = tf.cast(image, dtype=tf.float32) / 255.0\n",
    "    attention_image = rescaled_image * attentions[0]\n",
    "    attention_image = tf.image.rgb_to_grayscale(attention_image)\n",
    "    attention_image = tf.squeeze(attention_image, axis=-1)\n",
    "    attention_image = plt.cm.viridis(attention_image)\n",
    "    attention_image = PIL.Image.fromarray(np.uint8(attention_image * 255))\n",
    "    return attention_image\n",
    "\n",
    "\n",
    "def make_prediction(image: PIL.Image):\n",
    "    \"\"\"\n",
    "    Make a single prediction using the given model and image.\n",
    "\n",
    "    Args:\n",
    "        model_path: The path to the model to load.\n",
    "        image: The image to predict.\n",
    "        n_images: The number of images to predict.\n",
    "\n",
    "    Returns:\n",
    "        The predictions and attention vectors.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = from_pretrained_keras(MODEL_CHECKPOINT)\n",
    "    model.compile()\n",
    "    model_config = model.get_layer(\"tf_vi_t_model\").get_config()\n",
    "    w_featmap = IMG_SIZE[0] // model_config[\"patch_size\"]\n",
    "    h_featmap = IMG_SIZE[1] // model_config[\"patch_size\"]\n",
    "\n",
    "    # Convert images to tensorflow Dataset\n",
    "    image = image.convert(\"RGB\")\n",
    "    permutation = lambda image: tf.transpose(image, perm=[2, 0, 1])\n",
    "    image_preprocessor = get_image_preprocessor()\n",
    "    image_tf = permutation(image_preprocessor(image))\n",
    "    image_shape = tf.constant(image).shape\n",
    "    image_ds = tf.data.Dataset.from_tensors(image_tf).batch(1)\n",
    "\n",
    "    # Make predictions\n",
    "    model_output = model.predict(image_ds, verbose=0)\n",
    "\n",
    "    predictions = model_output[0]\n",
    "    predictions = [float(prediction) for prediction in predictions]\n",
    "    predicted_classes = [1 if pred > THRESHOLD else 0 for pred in predictions]\n",
    "\n",
    "    # Obtain the attention vector\n",
    "    attentions = get_attention(\n",
    "        model_output[1],\n",
    "        1,\n",
    "        model_config[\"num_attention_heads\"],\n",
    "        h_featmap,\n",
    "        w_featmap,\n",
    "        image_shape[0],\n",
    "        image_shape[1],\n",
    "    )\n",
    "\n",
    "    # Get the attention image\n",
    "    attention_image = get_attention_image(attentions, image)\n",
    "\n",
    "    return (attention_image, {IMG_CLASSES[predicted_classes[0]]: predictions[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KAGGLE_NOTEBOOK = \"[![Static Badge](https://img.shields.io/badge/Open_Notebook_in_Kaggle-blue?logo=kaggle&logoColor=white&labelColor=gray)](https://www.kaggle.com/code/mmenendezg/pneumonia-classifier-using-vit)\"\n",
    "GITHUB_REPOSITORY = \"[![Static Badge](https://img.shields.io/badge/Git_Repository-purple?logo=github&logoColor=white&labelColor=gray)](https://github.com/mmenendezg/pneumonia_x_ray)\"\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    gr.Markdown(\n",
    "    f\"\"\"\n",
    "    # Pneumonia Classifier\n",
    "\n",
    "    This is a space to test the Pneumonia Classifier model.\n",
    "\n",
    "    {KAGGLE_NOTEBOOK}\n",
    "\n",
    "    {GITHUB_REPOSITORY}\n",
    "    \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            uploaded_image = gr.Image(\n",
    "                label=\"Chest X-ray image\",\n",
    "                sources=[\"upload\", \"clipboard\"],\n",
    "                type=\"pil\",\n",
    "                height=550,\n",
    "            )\n",
    "        with gr.Column():\n",
    "            labels = gr.Label(label=\"Prediction\")\n",
    "            attention_image = gr.Image(\n",
    "                label=\"Attention zones\", image_mode=\"L\", height=425\n",
    "            )\n",
    "    with gr.Row():\n",
    "        classify_btn = gr.Button(\"Classify\", variant=\"primary\")\n",
    "        clear_btn = gr.ClearButton(components=[uploaded_image, labels, attention_image])\n",
    "    classify_btn.click(\n",
    "        fn=make_prediction, inputs=uploaded_image, outputs=[attention_image, labels]\n",
    "    )\n",
    "demo.launch(debug=True, inline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
