{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e98a2530-43fe-4424-bc20-7bbf57fd5a05",
    "_uuid": "db1f3958-d60a-47dc-92ef-5cca23bc8116"
   },
   "source": [
    "# Uploading the `Chest X-Ray` Dataset to Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29a5eda8-0c6c-4633-b35a-c5cc8009bd07",
    "_uuid": "c38f5a8c-feca-48ae-9255-60c8c4406836"
   },
   "source": [
    "This notebook's goal is to preprocess and upload the dataset [Chest X-Ray Images](https://data.mendeley.com/datasets/rscbjbr9sj) to the **Hugging Face Hub**. \n",
    "\n",
    "There will be two version of this dataset. The first is a raw version of the images as provided by the **Guangzhou Women and Children's Medical Center** of the **University of California San Diego**. The second will be a preprocessed version of the dataset. \n",
    "\n",
    "You can also find this dataset availble in [Kaggle](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7af4596f-16dc-46fa-957f-1512df5610a7",
    "_uuid": "38b0a1e5-4116-4399-906b-675002d8b82f"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc7bcd44-4615-4f92-92bf-93d9338efdfe",
    "_uuid": "0ed196eb-98fa-466f-8aec-a08692679f67"
   },
   "source": [
    "Let's first import the libraries we will use and create the constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "563302fe-ea5a-47f5-8a8b-81e9740d97c9",
    "_uuid": "f8f83dbb-7426-4eda-b4f4-7906753c7982",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmenendezg/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras import utils\n",
    "\n",
    "# Hugging Face\n",
    "import datasets\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "9213cf32-a48d-4e11-9ade-82daf13c97fc",
    "_uuid": "32fd03d3-f476-4f47-9f60-ff8027d61801",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Local paths that store the files\n",
    "RAW_DATA_PATH = \"../data/raw/pneumonia_xray/\"\n",
    "DATA_PATH = \"../data/processed/\"\n",
    "TFRECORDS_PATH = \"../data/processed/\"\n",
    "\n",
    "NAME_RAW_DATASET = \"mmenendezg/raw_pneumonia_x_ray\"\n",
    "NAME_DATASET = \"mmenendezg/pneumonia_x_ray\"\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "IMG_SIZE = (500, 500)\n",
    "CLASSES = [\"Normal\", \"Pneumonia\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "255a7bf1-57ac-4d75-95f9-a5628f6f7a20",
    "_uuid": "ba696893-9242-4fb7-827d-413cc957a837"
   },
   "source": [
    "## Dataset Info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eed0d387-1dce-4581-b116-c539e004efe4",
    "_uuid": "0b3919ea-cf49-4dca-8287-34dd001f6368"
   },
   "source": [
    "The dataset contains X-Ray chest images from independent patients. The images are classified into two classes:\n",
    "\n",
    ">- 0: Normal\n",
    ">- 1: Pneumonia\n",
    "\n",
    "The shape, aspect ratio and size of the images vary. There are images with 3 channels of color (i.e., RGB color), and other images that have no channel (i.e., grayscale. The only channel is implicit). It is important to take this into consideration when preprocessing the dataset.\n",
    "\n",
    "The structure of the folders of the original dataset is the following:\n",
    "\n",
    "```\n",
    "|-- pneumonia_x_ray\n",
    "   |-- train\n",
    "      |-- normal\n",
    "      |-- pneumonia\n",
    "   |-- test\n",
    "      |-- normal\n",
    "      |-- pneumonia\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51caa7cf-5c17-4f12-b874-f898e0c927fc",
    "_uuid": "44993e74-e20e-45db-94e6-29eb514492d0"
   },
   "source": [
    "## Create Raw dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f836b13a-9530-4be5-b67e-b2d33ae11226",
    "_uuid": "54e3e242-53ba-4526-920e-f00bd617f1f2"
   },
   "source": [
    "The first version of the dataset will be a raw version of the dataset. This will provide more flexibility to preprocess the images according to the project needs. \n",
    "\n",
    "It is important to login to Hugging Face to upload the dataset. In the code below change the `[TOKEN]` for the one provided by [Hugging Face](https://huggingface.co/settings/tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "c804884d-b809-47a4-9a34-cc8ae5cac322",
    "_uuid": "cf0f4b42-bf06-43a2-9d80-4e86f962668d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/mmenendezg/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8d6291e-172b-4db8-9ed7-9ae5dcaa3d26",
    "_uuid": "c4bfb355-4e81-45cd-8cdf-eb2645e5e67d"
   },
   "source": [
    "The `load_dataset()` method allows us to download datasets stored in the Hugging Face Hub, or to load data stored locally. To load images from a local folder it is necessary to set `\"imagefolder\"` as the first argument, and the path of the folder containing the images in the `data_dir` argument. \n",
    "\n",
    "This will automatically identify the structure of the folders (see above) and create a `DatasetDict` containing the train and test splits, and it creates the two labels in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "1ae30ebf-b20a-45d8-8253-5be81231d11f",
    "_uuid": "f90a275f-5f9b-42ec-95e7-fab95695e7e3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 5232/5232 [00:00<00:00, 9132.68it/s] \n",
      "Resolving data files: 100%|██████████| 624/624 [00:00<00:00, 15737.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /Users/mmenendezg/.cache/huggingface/datasets/imagefolder/default-5af3f26d12a08b80/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 5232/5232 [00:00<00:00, 144681.71it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Downloading data files: 100%|██████████| 624/624 [00:00<00:00, 127137.17it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/mmenendezg/.cache/huggingface/datasets/imagefolder/default-5af3f26d12a08b80/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 112.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 5232\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 624\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = datasets.load_dataset(\"imagefolder\", data_dir=RAW_DATA_PATH)\n",
    "raw_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35c0b9d5-888a-417f-a144-f550ac1fd82c",
    "_uuid": "817a9940-cee1-41fe-a93a-5db574669782"
   },
   "source": [
    "Once we have loaded the data, we can push the dataset to the hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "80d02856-b432-4b29-8345-118269a060ef",
    "_uuid": "0a9fcccf-6a29-4e90-a75d-1760c2eb37bb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 16.00ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 23.82ba/s]89s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 219.84ba/s]0s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 178.44ba/s]5s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 248.73ba/s]5s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 222.01ba/s]4s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 224.73ba/s]4s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 7/7 [01:33<00:00, 13.31s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 7/7 [00:01<00:00,  6.97it/s]\n",
      "Pushing split test to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 87.89ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:07<00:00,  7.93s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_dataset.push_to_hub(NAME_RAW_DATASET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4504a926-12e5-471d-ba1c-aa6bd9fc6f63",
    "_uuid": "939e210d-ebed-4058-9a40-e539df4e676f"
   },
   "source": [
    "The dataset has been successfully uploaded to the [Hub](https://huggingface.co/datasets/mmenendezg/raw_pneumonia_x_ray). We can now download the data using the same name set when pushing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "4bbbf06f-126a-4cb5-996c-dae9d8151fd0",
    "_uuid": "b7426ba3-21d1-41e4-b65c-abacf2fafaf8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 550/550 [00:00<00:00, 2.21MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /Users/mmenendezg/.cache/huggingface/datasets/mmenendezg___parquet/mmenendezg--raw_pneumonia_x_ray-406f309309d5d4c7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 458M/458M [00:15<00:00, 29.2MB/s]\n",
      "Downloading data: 100%|██████████| 373M/373M [00:12<00:00, 29.1MB/s]\n",
      "Downloading data: 100%|██████████| 61.2M/61.2M [00:02<00:00, 28.0MB/s]\n",
      "Downloading data: 100%|██████████| 59.5M/59.5M [00:02<00:00, 28.0MB/s]\n",
      "Downloading data: 100%|██████████| 58.2M/58.2M [00:02<00:00, 27.9MB/s]\n",
      "Downloading data: 100%|██████████| 71.8M/71.8M [00:02<00:00, 28.2MB/s]\n",
      "Downloading data: 100%|██████████| 70.7M/70.7M [00:02<00:00, 28.0MB/s]\n",
      "Downloading data: 100%|██████████| 111M/111M [00:03<00:00, 28.9MB/s]]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:50<00:00, 25.23s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 400.33it/s]\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /Users/mmenendezg/.cache/huggingface/datasets/mmenendezg___parquet/mmenendezg--raw_pneumonia_x_ray-406f309309d5d4c7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 47.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 5232\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 624\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pneumonia_x_ray = datasets.load_dataset(NAME_RAW_DATASET)\n",
    "pneumonia_x_ray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37bae84d-2016-473d-8754-7f7065867649",
    "_uuid": "d1a64c2f-fe3f-4111-8f74-6254cde0a3ba"
   },
   "source": [
    "## Create Preprocessed Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f56ab437-f889-4253-94f1-caba9e7a0a2f",
    "_uuid": "ffb36047-70f8-4b49-bda7-892738c0feed"
   },
   "source": [
    "The raw images gives us a lot of flexibility to work with the data, but this creates some challenges when preprocessing the images to train a model. \n",
    "\n",
    "The second version of the dataset contains preprocessed images that solves the 2 main challenges of the raw data:\n",
    "\n",
    "> - It converts all images to RGB (i.e., All the images have 3 channels)\n",
    "> - It resizes them to a fixed size, and therefore, a fixed aspect ratio (1:1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f89c7451-424c-4a8a-9e93-6d42f228d6ca",
    "_uuid": "1c45161e-eaab-49cb-8bea-3516f0e2b32e"
   },
   "source": [
    "### Preprocess the images with TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "940c5846-483c-4dc5-971d-818a27693f44",
    "_uuid": "74ec60bd-2bd1-4408-b3ad-ab04dc713d1d"
   },
   "source": [
    "It is necessary to preprocess the images. TensorFlow offers a wide variety of methods to load and preprocess images. The method `tf.keras.utils.image_dataset_from_directory()` provides all the preprocessing we need. Additionally, it infers the labels of the images based on the structure of the folders.\n",
    "\n",
    "When resizing the images, it is able to uses different interpolation methods. For this dataset we will use the `nearest` option that uses *k-nearest neighbors* algorithm to calculate the size of every pixel. This gives as result pixel values that are integers, and we do not need to process float values higher than 1. When working with images, it asumes that if the `dtype` of the tensors are float, the values should be betweeen 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "94065245-2fa5-4f13-90d1-d441b94d804b",
    "_uuid": "4d105237-54d4-4b98-964f-4048e066a591",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(path: str, shuffle: bool = False) -> tf.data.Dataset:\n",
    "    \"\"\"Loads a dataset of images from a directory.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the directory containing the images.\n",
    "        shuffle: Whether to shuffle the dataset.\n",
    "\n",
    "    Returns:\n",
    "        A `tf.data.Dataset` of images and labels.\n",
    "    \"\"\"\n",
    "    dataset = utils.image_dataset_from_directory(\n",
    "        path,\n",
    "        interpolation=\"nearest\",\n",
    "        image_size=IMG_SIZE,\n",
    "        label_mode=\"int\",\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=None,\n",
    "        shuffle=shuffle,\n",
    "        class_names=[\"normal\", \"pneumonia\"],\n",
    "    )\n",
    "    return dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "72cbc629-3492-45f9-929b-3541a6b1dd3c",
    "_uuid": "8eea400c-e21b-42ec-bd2e-7327da45527e"
   },
   "source": [
    "Let's load the images from the raw data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "9d863417-c3d1-43df-a26d-e56d28e2cb3e",
    "_uuid": "5d25ae85-e8e7-466f-a97e-419337d07d6c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5232 files belonging to 2 classes.\n",
      "Found 624 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = tf.io.gfile.join(RAW_DATA_PATH, \"train\")\n",
    "test_path = tf.io.gfile.join(RAW_DATA_PATH, \"test\")\n",
    "\n",
    "train_ds = load_dataset(train_path, shuffle=True)\n",
    "test_ds = load_dataset(test_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset only has `train` and `test` sets. Let's create a validation split set to upload it to the hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(dataset: tf.data.Dataset, ratio: float = 0.2):\n",
    "    # Calculate the number of images per label for the valid set\n",
    "    normal_images = int(1349 * ratio)\n",
    "    pneumo_images = int(3883 * ratio)\n",
    "\n",
    "    # Separate the dataset per labels\n",
    "    normal_ds = dataset.filter(lambda image, label: label == 0)\n",
    "    pneumo_ds = dataset.filter(lambda image, label: label == 1)\n",
    "\n",
    "    # Separate the datasets per splits\n",
    "    normal_train = normal_ds.skip(normal_images)\n",
    "    normal_valid = normal_ds.take(normal_images)\n",
    "\n",
    "    pneumo_train = pneumo_ds.skip(pneumo_images)\n",
    "    pneumo_valid = pneumo_ds.take(pneumo_images)\n",
    "\n",
    "    # Concatenate the datasets\n",
    "    train_ds = normal_train.concatenate(pneumo_train).shuffle(5000)\n",
    "    valid_ds = normal_valid.concatenate(pneumo_valid)\n",
    "\n",
    "    return train_ds, valid_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = create_validation_set(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4187\n",
      "Valid: 1045\n",
      "Test: 624\n"
     ]
    }
   ],
   "source": [
    "n_train = 0\n",
    "for _, _ in train_ds:\n",
    "    n_train += 1\n",
    "\n",
    "n_valid = 0\n",
    "for _, _ in valid_ds:\n",
    "    n_valid += 1\n",
    "\n",
    "n_test = 0\n",
    "for _, _ in test_ds:\n",
    "    n_test += 1\n",
    "\n",
    "print(f\"Train: {n_train}\")\n",
    "print(f\"Valid: {n_valid}\")\n",
    "print(f\"Test: {n_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "13f11a7c-93ce-47c5-8543-f0449bc823f8",
    "_uuid": "61599bcc-0b85-4f1a-8f21-0544d5737fcb"
   },
   "source": [
    "Once the images have been processed, we need to convert them to `datasets.Dataset` (Hugging Face dataset object). The `datasets` library does not provide an specific method to create a dataset from `tf.data.Dataset`. There are several ways to achieve this: using a generator, creating a dictionary, converting the dataset to a pandas dataframe, etc. In this case we will save the dataset to images in a local folder, and then we wil load the dataset using the `dataset.load_dataset()` method as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image_array: np.array, filepath: str):\n",
    "    \"\"\"Saves an image to a file.\n",
    "\n",
    "    Args:\n",
    "        image_array: The image array to save.\n",
    "        filepath: The path to the file to save the image to.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    image = Image.fromarray(image_array)\n",
    "    image.save(filepath)\n",
    "\n",
    "\n",
    "def save_images(dataset: tf.data.Dataset, set_type: str = \"train\"):\n",
    "    \"\"\"Saves images from a dataset to a directory.\n",
    "\n",
    "    Args:\n",
    "        dataset: A `tf.data.Dataset` of images and labels.\n",
    "        set_type: The type of dataset, either `\"train\"`, `\"validation\"` or `\"test\"`.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    id_images = [0, 0]\n",
    "    classes = [\"normal\", \"pneumonia\"]\n",
    "    paths = [\n",
    "        f\"../data/processed/{set_type}/{classes[0]}\",\n",
    "        f\"../data/processed/{set_type}/{classes[1]}\",\n",
    "    ]\n",
    "    for path in paths:\n",
    "        tf.io.gfile.makedirs(path)\n",
    "\n",
    "    for idx, (image, label) in enumerate(dataset):\n",
    "        id_image = id_images[label.numpy()]\n",
    "        image_path = tf.io.gfile.join(\n",
    "            paths[label.numpy()], f\"{set_type}-{id_image}.jpeg\"\n",
    "        )\n",
    "        save_image(image.numpy(), image_path)\n",
    "        id_images[label.numpy()] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "71b69984-f5fa-43cd-a38b-e4608c13d3c2",
    "_uuid": "e98e3645-2720-405d-a96d-954962a48822",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "save_images(train_ds, \"train\")\n",
    "save_images(valid_ds, \"validation\")\n",
    "save_images(test_ds, \"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bb05d2f2-f20b-42f0-8d69-be155626d369",
    "_uuid": "d788189a-8246-4b90-898e-3a676320af82"
   },
   "source": [
    "### Upload preprocessed dataset to Hugging Face Hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "745aacd6-1a62-416b-95e6-c877733f1330",
    "_uuid": "77eedf28-9730-4c16-9a68-34c067ac03d1"
   },
   "source": [
    "It is possible to load the whole dataset in a single line of code, but for a reason I was not able to find, this causes to duplicate the images. One alternative, is to load the sets separate and then to create a `datasets.DatasetDict` object containg both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "01248515-8c90-45a7-a7da-21ac0d632af3",
    "_uuid": "1f7d0284-5523-476d-9018-19255f664e43",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_and_upload_dataset_hf():\n",
    "    \"\"\"Creates and uploads a dataset to the Hugging Face Hub.\n",
    "\n",
    "    This function creates a dataset from two directories, one for training and one for testing. \n",
    "    The dataset is then uploaded to the Hugging Face Hub.\n",
    "    \"\"\"\n",
    "    train_path = tf.io.gfile.join(DATA_PATH, \"train\")\n",
    "    valid_path = tf.io.gfile.join(DATA_PATH, \"validation\")\n",
    "    test_path = tf.io.gfile.join(DATA_PATH, \"test\")\n",
    "\n",
    "    train_dataset = datasets.load_dataset(\"imagefolder\", data_dir=train_path, split=\"train\")\n",
    "    valid_dataset = datasets.load_dataset(\"imagefolder\", data_dir=valid_path, split=\"validation\")\n",
    "    test_dataset = datasets.load_dataset(\"imagefolder\", data_dir=test_path, split=\"test\")\n",
    "    dataset = datasets.DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": valid_dataset,\n",
    "        \"test\": test_dataset\n",
    "    })\n",
    "    dataset.push_to_hub(NAME_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "9a049310-aeee-4ab1-852c-5e16256dd135",
    "_uuid": "3cbf5d77-deec-44b9-9eb6-607f11a34f9c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 4187/4187 [00:00<00:00, 9570.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /Users/mmenendezg/.cache/huggingface/datasets/imagefolder/default-8622b0c8491092e5/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 4187/4187 [00:00<00:00, 145670.10it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/mmenendezg/.cache/huggingface/datasets/imagefolder/default-8622b0c8491092e5/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 1045/1045 [00:00<00:00, 11889.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /Users/mmenendezg/.cache/huggingface/datasets/imagefolder/default-21a350812d50de72/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1045/1045 [00:00<00:00, 141814.08it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/mmenendezg/.cache/huggingface/datasets/imagefolder/default-21a350812d50de72/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 624/624 [00:00<00:00, 19072.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /Users/mmenendezg/.cache/huggingface/datasets/imagefolder/default-42bef916376ca0e4/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 624/624 [00:00<00:00, 135622.64it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Pushing split train to the Hub.                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/mmenendezg/.cache/huggingface/datasets/imagefolder/default-42bef916376ca0e4/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 1c4ffe38-4829-4866-998e-ed869f9a8749)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    462\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/urllib3/util/retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 550\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    551\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/urllib3/packages/six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    770\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    462\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_and_upload_dataset_hf()\n",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36mcreate_and_upload_dataset_hf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m test_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mload_dataset(\u001b[39m\"\u001b[39m\u001b[39mimagefolder\u001b[39m\u001b[39m\"\u001b[39m, data_dir\u001b[39m=\u001b[39mtest_path, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mDatasetDict({\n\u001b[1;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: train_dataset,\n\u001b[1;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m: valid_dataset,\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m: test_dataset\n\u001b[1;32m     18\u001b[0m })\n\u001b[0;32m---> 19\u001b[0m dataset\u001b[39m.\u001b[39;49mpush_to_hub(NAME_DATASET)\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/datasets/dataset_dict.py:1635\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[0;34m(self, repo_id, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   1633\u001b[0m logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPushing split \u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m to the Hub.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1634\u001b[0m \u001b[39m# The split=key needs to be removed before merging\u001b[39;00m\n\u001b[0;32m-> 1635\u001b[0m repo_id, split, uploaded_size, dataset_nbytes, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[split]\u001b[39m.\u001b[39;49m_push_parquet_shards_to_hub(\n\u001b[1;32m   1636\u001b[0m     repo_id,\n\u001b[1;32m   1637\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m   1638\u001b[0m     private\u001b[39m=\u001b[39;49mprivate,\n\u001b[1;32m   1639\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1640\u001b[0m     branch\u001b[39m=\u001b[39;49mbranch,\n\u001b[1;32m   1641\u001b[0m     max_shard_size\u001b[39m=\u001b[39;49mmax_shard_size,\n\u001b[1;32m   1642\u001b[0m     num_shards\u001b[39m=\u001b[39;49mnum_shards\u001b[39m.\u001b[39;49mget(split),\n\u001b[1;32m   1643\u001b[0m     embed_external_files\u001b[39m=\u001b[39;49membed_external_files,\n\u001b[1;32m   1644\u001b[0m )\n\u001b[1;32m   1645\u001b[0m total_uploaded_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m uploaded_size\n\u001b[1;32m   1646\u001b[0m total_dataset_nbytes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dataset_nbytes\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:5223\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub\u001b[0;34m(self, repo_id, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5220\u001b[0m     organization_or_username \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mwhoami(token)[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   5221\u001b[0m     repo_id \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00morganization_or_username\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 5223\u001b[0m api\u001b[39m.\u001b[39;49mcreate_repo(\n\u001b[1;32m   5224\u001b[0m     repo_id,\n\u001b[1;32m   5225\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   5226\u001b[0m     repo_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   5227\u001b[0m     private\u001b[39m=\u001b[39;49mprivate,\n\u001b[1;32m   5228\u001b[0m     exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   5229\u001b[0m )\n\u001b[1;32m   5231\u001b[0m \u001b[39m# Find decodable columns, because if there are any, we need to:\u001b[39;00m\n\u001b[1;32m   5232\u001b[0m \u001b[39m# embed the bytes from the files in the shards\u001b[39;00m\n\u001b[1;32m   5233\u001b[0m decodable_columns \u001b[39m=\u001b[39m (\n\u001b[1;32m   5234\u001b[0m     [k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m require_decoding(v, ignore_decode_attribute\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)]\n\u001b[1;32m   5235\u001b[0m     \u001b[39mif\u001b[39;00m embed_external_files\n\u001b[1;32m   5236\u001b[0m     \u001b[39melse\u001b[39;00m []\n\u001b[1;32m   5237\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2305\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware)\u001b[0m\n\u001b[1;32m   2303\u001b[0m     json[\u001b[39m\"\u001b[39m\u001b[39mlfsmultipartthresh\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lfsmultipartthresh  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   2304\u001b[0m headers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_hf_headers(token\u001b[39m=\u001b[39mtoken, is_write_action\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 2305\u001b[0m r \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39;49mpost(path, headers\u001b[39m=\u001b[39;49mheaders, json\u001b[39m=\u001b[39;49mjson)\n\u001b[1;32m   2307\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2308\u001b[0m     hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\u001b[39mself\u001b[39m, url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     request_id \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/Developer/Projects/pneumonia-xray/.venv/lib/python3.11/site-packages/requests/adapters.py:501\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    503\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    505\u001b[0m         \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 1c4ffe38-4829-4866-998e-ed869f9a8749)')"
     ]
    }
   ],
   "source": [
    "create_and_upload_dataset_hf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d66ffc7-d9fc-48f2-9f76-5e46dec7f32d",
    "_uuid": "2d55737a-6192-49eb-bc8b-deb649eda499"
   },
   "source": [
    "The dataset has been successfully uploaded to the [Hub](https://huggingface.co/datasets/mmenendezg/pneumonia_x_ray)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e7b2bf83-76a7-4834-b160-e5d1550a4491",
    "_uuid": "43e1b039-2613-4e8e-bc8e-683d16face86"
   },
   "source": [
    "## Images per Classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29162c76-5dfb-4d29-8cb0-efd00809502f",
    "_uuid": "01ad8510-7e0f-443b-ab90-d211c3431aff"
   },
   "source": [
    "Finally, lets see if the classes are balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9421ff01-6a3a-40b9-9b18-7c5cc03981bd",
    "_uuid": "d61507d6-3de4-4257-b4f9-63c263a1f5e2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_examples_per_class():\n",
    "    \"\"\"\n",
    "    Plots the distribution of examples per class in the train and test datasets.\n",
    "\n",
    "    The function calculates the count of examples for each class in the train and test datasets,\n",
    "    and then visualizes the distribution using a bar plot.\n",
    "    \"\"\"\n",
    "    count_train = [0, 0]\n",
    "    count_valid = [0, 0]\n",
    "    count_test = [0, 0]\n",
    "\n",
    "    for _, label in train_ds:\n",
    "        count_train[label.numpy()] += 1\n",
    "    \n",
    "    for _, label in valid_ds:\n",
    "        count_valid[label.numpy()] += 1\n",
    "\n",
    "    for _, label in test_ds:\n",
    "        count_test[label.numpy()] += 1\n",
    "    \n",
    "    counts = {\n",
    "        \"Classes\": [\"Normal\", \"Pneumonia\", \"Normal\", \"Pneumonia\", \"Normal\", \"Pneumonia\"],\n",
    "        \"Examples\": count_train + count_valid + count_test,\n",
    "        \"Set\": [\"Train\", \"Train\", \"Valid\", \"Valid\", \"Test\", \"Test\"],\n",
    "    }\n",
    "    data = pd.DataFrame.from_dict(counts)\n",
    "    ax = sns.barplot(data, x=\"Examples\", y=\"Set\", hue=\"Classes\", palette=\"magma\")\n",
    "    ax.bar_label(ax.containers[0])\n",
    "    ax.bar_label(ax.containers[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "39186571-e4c1-4642-b314-181f0876da3a",
    "_uuid": "df099075-503c-4e1a-b398-ff6bb3822f96",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_examples_per_class()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c75c055-9eb7-443f-8d3d-21f2a18c817b",
    "_uuid": "b81bdcdc-23c7-4e8a-9098-620e24017d48"
   },
   "source": [
    "The classes are unbalanced, specially the training set. Using the `accuracy` metric to evaluate a model trained on this dataset may not be the best option, and a better choice would be to use `recall`, `precision` or `F1-Score` metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e1c51b6-5bb8-4ba4-8d51-5b90972f5fcd",
    "_uuid": "fca61254-e3b6-4637-bde0-012b5316988e"
   },
   "source": [
    "## References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a3e60079-a4eb-49b8-950d-f9b6e896e265",
    "_uuid": "b0f8f660-f2ca-4b59-89bd-d34816a88a87"
   },
   "source": [
    ">**References**:\n",
    ">\n",
    "> - Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), “Large Dataset of Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images”, Mendeley Data, V3, doi: 10.17632/rscbjbr9sj.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
